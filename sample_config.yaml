# config.yaml
# ðŸ¦Ž Calango AI Configuration File
# Import this file via the "Settings" (A Toca) tab to instantly setup your providers.

providers:
  # --- OPENAI (The Standard) ---
  openai:
    api_key: "${OPENAI_API_KEY}"
    models:
      # Generation 5 (Latest)
      - "gpt-5.2"         # Flagship. Best for complex coding & agentic tasks.
      - "gpt-5.1"         # "Warmer" personality. Great for creative writing & general chat.
      - "gpt-5-mini"      # Budget King. Replaces 4o-mini. Fast & very cheap.

      # Reasoning (Thinking Models)
      - "o3-pro"          # Deepest thinker. Slow, but solves complex math/science problems.
      - "o3"              # Assertive reasoning. Faster than Pro, good for logic puzzles.
      - "o1"              # Previous gen reasoning. Good fallback.

      # Legacy / Reliable
      - "gpt-4o"          # The 2024 standard. Reliable, well-understood behavior.

  # --- ANTHROPIC (Claude) ---
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    models:
      # Generation 4.5 (Latest)
      - "claude-sonnet-4-5" # Best All-Rounder. Excellent at coding & nuance.
      - "claude-opus-4-5"   # Max Intelligence. Best for long-form writing & deep analysis.
      - "claude-haiku-4-5"  # Speed Demon. Extremely cheap, perfect for quick chats.

  # --- GOOGLE (Gemini) ---
  gemini:
    api_key: "${GEMINI_API_KEY}"
    models:
      # Generation 3 (Frontier)
      - "gemini-3-pro-preview"   # New reasoning engine with "Thought Signatures".
      - "gemini-3-flash-preview" # Frontier intelligence at high speed.

      # Generation 2.5 (Stable Workhorses)
      - "gemini-2.5-pro"    # Stable production model. Reliable for business tasks.
      - "gemini-2.5-flash"  # High volume / Low cost. Good for summarizing long docs.

  # --- GROQ (Open Source Speed) ---
  # Use this provider for Llama models running at extreme speeds
  groq:
    api_key: "${GROQ_API_KEY}"
    models:
      # Meta Llama 3 Series
      - "llama-3.3-70b-versatile" # Powerhouse. GPT-4 class performance, open weights.
      - "llama-3.1-8b-instant"    # Instant. The fastest model available. Good for simple queries.
      - "mixtral-8x7b-32768"      # Large Context. Good for processing large text blocks quickly.

  # --- OLLAMA (Local / Privacy) ---
  # If you are running Ollama locally (http://localhost:11434)
  # You might need to set the API Base URL in the code if not default
  ollama:
    api_key: "ollama"  # Dummy key
    models:
      - "llama3"   # Standard Llama 3 running locally.
      - "mistral"  # Mistral 7B. Efficient and smart for its size.
      - "gemma2"   # Google's open model. Good for creative tasks.