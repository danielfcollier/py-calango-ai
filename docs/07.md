### ðŸ”­ Phase 7: Observability & Tracing - Execution Plan

**Before you begin:**

1. Run `pip install langfuse`.
2. Sign up for a free Langfuse account (or self-host) and get your `PK` (Public Key), `SK` (Secret Key), and `HOST` (e.g., `https://cloud.langfuse.com`).
3. Add these to your `.env` file:
```env
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com

```



---

#### ðŸ”¹ Prompt 1: The Observer (Tracing Service)

*First, we create the wrapper that handles communication with the observability platform.*

> **Context:**
> I am adding LLMOps to my application using **Langfuse**. I need a service to handle tracing and scoring.
> **Goal:**
> Create `src/calango/services/tracing_service.py`.
> **Requirements:**
> 1. **Class `TracingService**`:
> * **Init**: Initialize the `Langfuse` client using environment variables.
> * **Method `create_trace(session_id, user_id, tags)**`:
> * Creates a new trace for a conversation turn.
> * Returns the trace object.
> 
> 
> * **Method `log_generation(trace, model, messages, response, usage, cost)**`:
> * Adds a "generation" span to the trace.
> * logs the inputs (messages), output (response), and metadata (model, tokens, cost).
> 
> 
> * **Method `submit_feedback(trace_id, value, comment=None)**`:
> * Sends a score (e.g., 1 for Thumbs Up, 0 for Thumbs Down) to the specific trace.
> 
> 
> 
> 
> 
> 
> Please generate the code for `src/calango/services/tracing_service.py`.

---

#### ðŸ”¹ Prompt 2: Instrumentation (Hooking the Brain)

*Now we attach the observer to our Engine or Chat Service.*

> **Context:**
> I have the `TracingService`. Now I need to capture the data from my `ChatService` and `CalangoEngine`.
> **Goal:**
> Update `src/calango/services/chat_service.py` to log traces.
> **Requirements:**
> 1. **Inject Dependencies**: Add `tracing_service` to `ChatService.__init__`.
> 2. **Update `send_message**`:
> * **Start Trace**: Before calling the engine, call `tracing_service.create_trace(session_id=..., user_id=...)`.
> * **Pass Trace ID**: If `CalangoEngine` supports callbacks, pass the trace. If not, we will wrap the result after.
> * **Log Generation**: After getting the response (full text) from the engine, call `tracing_service.log_generation(...)`.
> * **Return Trace ID**: Ensure `send_message` returns the `trace_id` along with the response content, so the UI can use it for feedback buttons.
> 
> 
> 
> 
> Please generate the updated `src/calango/services/chat_service.py`.

---

#### ðŸ”¹ Prompt 3: The Feedback Loop (UI)

*Finally, we let the user tell us if the AI was smart or dumb.*

> **Context:**
> My backend is tracing execution. I need to allow users to rate the AI responses in the Streamlit UI.
> **Goal:**
> Update `src/ui/home.py` to add Feedback buttons.
> **Requirements:**
> 1. **State Management**: We need to store `trace_id` for each assistant message in `st.session_state.messages`.
> 2. **UI Update**:
> * Inside the message loop (`for msg in messages:`), if the role is 'assistant':
> * Render the message content.
> * Below the content, use `st.feedback("thumbs")` (if Streamlit >= 1.37) or two columns with buttons (`ðŸ‘`, `ðŸ‘Ž`).
> * **Callback**: When a button is clicked, call `chat_service.tracing_service.submit_feedback(msg['trace_id'], score)`.
> * Show a `st.toast("Obrigado pelo feedback!")`.
> 
> 
> 
> 
> 
> 
> Please generate the updated logic for `src/ui/home.py`.